{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9ea3ab0",
   "metadata": {},
   "source": [
    "# ６章. 学習に関するテクニック"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43da97db",
   "metadata": {},
   "source": [
    "## 6-1. パラメータの更新"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c634c2",
   "metadata": {},
   "source": [
    "最適化問題は難しい。確率的勾配降下法（SGD）は単純で、パラメータ空間を闇雲に探すよりも賢い方法だった。しかし、解決したい問題によってはSDGよりもさらにスマートな手法が存在する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f80f2c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "# SDG クラスの実装\n",
    "\n",
    "class SDG:\n",
    "    \n",
    "    def __init__(self, lr=0.01):  # lr(learning rate) : 学習係数をインスタンスに持つ\n",
    "        self.lr = lr\n",
    "        \n",
    "    def upgrade(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f84441",
   "metadata": {},
   "source": [
    "SDGは関数の形状が等方的でないと、非効率な経路になる。\n",
    "\n",
    "Momentum\n",
    "\n",
    "モーメンタムとは運動量を意味し、物理と関係がある。\n",
    "\n",
    "数式：v←av＊ｎ＊dL/dW, W←W＋v\n",
    "\n",
    "dL/dW：Wに対する損失関数の勾配\n",
    "\n",
    "W：更新する重みパラメータ\n",
    "\n",
    "n：学習係数\n",
    "\n",
    "v：物理でいうところの速度\n",
    "\n",
    "av：物体が何も力を受けないときに徐々に減速するための役割を担う，物理でいう地面の摩擦や空気抵抗\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f97ed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "# Momentum の実装\n",
    "\n",
    "class Momentum:\n",
    "    \n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum*self.v[key] - self.lr * grads[key]\n",
    "            params[key] += self.v[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd01f75f",
   "metadata": {},
   "source": [
    "Momentumを使って最適化問題を解くと、SDGよりも早く(0, 0)に向かって収束している様子が確認できる。これはx軸方向に受ける力は小さくても常に同じ方向の力を受けることで加速するためである。y軸方向は正と負の方向の力を交互に受けるため、それらは互いに打ち消し合い、y軸方向の速度が安定しない。これらによってSDGと比較してx軸方向へ早く進み、(0, 0)への収束が早まる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed29b70",
   "metadata": {},
   "source": [
    "AdaGrad\n",
    "\n",
    "学習係数の減衰：学習が進むについて学習係数を小さくすること。最初に大きく学習し、次第に小さく学習するという手法。\n",
    "\n",
    "AdaGradは一つ一つのパラメータに対して適応的に学習係数を調整しながら学習を行う手法。\n",
    "\n",
    "数式：h←h＋dL/dW 二乗和 dL/DW,  W←W-n * 1/sqrt(h) * dL/dW\n",
    "\n",
    "h：これまで経験した勾配の値を２乗和として保持する。パラメータの更新の際にかけ合わせている、1/sqrt(h)を乗算することは学習スケールの調整を担っており、これはパラメータの要素の中で大きく更新された要素は学習係数が次第に小さくなるという学習係数の減衰をパラメータの要素ごとに行うことができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ee9e602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding;utf-8 -*-\n",
    "\n",
    "# AdaGrad の実装\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class AdaGrad:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)  # 1e-7　ゼロ除算を防ぐ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05741120",
   "metadata": {},
   "source": [
    "Adam\n",
    "\n",
    "Momentumは、ボールがお椀を転がるように物理法則に準じる動きを表現し、AdaGradは、パラメータの要素ごとに適応的に更新ステップを調整した。この２つの手法を融合したすることがAdamのベースとなるアイデア。\n",
    "\n",
    "Adamは2015年に提案された新しい手法であり、理論は複雑だが、先の２つの手法の利点を組み合わせることで効率的にパラメータ空間を探索することが期待される。\n",
    "\n",
    "Adamは３つのハイパーパラメータを設定する。１．学習係数、２．一次モーメント用の係数b1、３．二次モーメント用の係数b2である。論文によるとb1は0.9, b2は0.999であり、その設定値は多くの場合うまく行くらしい。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaeb59b",
   "metadata": {},
   "source": [
    "MNISTデータセットによる更新手法の比較\n",
    "\n",
    "５層ニューラルネットワーク、各層１００個のニューロン、活性化関数はReLUを使用したネットワーク。\n",
    "\n",
    "（ソースコードは配賦済み）\n",
    "\n",
    "実験は学習係数のハイパーパラメータや、ニューラルネットワークの構造（何層の深さかなど）によって結果は変化する。ただし、一般的にはSDGよりも他の３つの手法が早く学習でき、時には最終的な認識性能も高くなる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30b87bf",
   "metadata": {},
   "source": [
    "## 6-2. 重みの初期値"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4fd2f1",
   "metadata": {},
   "source": [
    "## 6-3. Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5507407",
   "metadata": {},
   "source": [
    "## 6-4. 正則化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75476265",
   "metadata": {},
   "source": [
    "## 6-5. ハイパーパラメータの検証"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f298c366",
   "metadata": {},
   "source": [
    "## 6-6. まとめ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a893594d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
